{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import argparse\n",
    "import pickle \n",
    "import os\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption import caption_image_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('./data/annotations/captions_flickr8k.json')\n",
    "# coco = COCO('./data/annotations/captions_flickr30k.json')\n",
    "# coco = COCO('./data/annotations/captions_val2014.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = set()\n",
    "for k in coco.anns.keys():\n",
    "    img_ids.add(coco.anns[k]['image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for img_id in img_ids:\n",
    "    images.append((coco.loadImgs(img_id)[0]['file_name'], coco.loadImgs(img_id)[0]['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint = torch.load(args['model'])\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder = decoder.to(device)\n",
    "    decoder.eval()\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "    \n",
    "    # Load word map (word2idx)\n",
    "    with open(args['word_map'], 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "        \n",
    "    # idx2word\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}  \n",
    "    \n",
    "    for i, (image_path, image_id) in enumerate(images):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, 'th image')\n",
    "        \n",
    "        seq, alphas = caption_image_beam_search(encoder, decoder, 'data/Flickr8k/Flicker8k_Dataset/'+image_path, word_map, 4)\n",
    "#         seq, alphas = caption_image_beam_search(encoder, decoder, 'data/Flickr30k/flickr30k-images/'+image_path, word_map, 4)\n",
    "#         seq, alphas = caption_image_beam_search(encoder, decoder, 'data/val2014/'+image_path, word_map, 4)\n",
    "        \n",
    "        sampled_caption = []\n",
    "        for ind in seq[1:]:\n",
    "            word = rev_word_map[ind]\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            sampled_caption.append(word)\n",
    "            \n",
    "        sentence = ' '.join(sampled_caption)\n",
    "        \n",
    "        results.append({\n",
    "            'image_id': image_id,\n",
    "            'caption': sentence\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model':'BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar',\n",
    "    'word_map':'data/Flickr8k/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'\n",
    "}\n",
    "\n",
    "# args = {\n",
    "#     'model':'BEST_checkpoint_flickr30k_5_cap_per_img_5_min_word_freq.pth.tar',\n",
    "#     'word_map':'data/Flickr30k/WORDMAP_flickr30k_5_cap_per_img_5_min_word_freq.json'\n",
    "# }\n",
    "\n",
    "# args = {\n",
    "#     'model':'models/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar',\n",
    "#     'word_map':'models/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th image\n"
     ]
    }
   ],
   "source": [
    "results =  main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 7000, 'caption': 'two brown dogs are playing in the snow'},\n",
       " {'image_id': 7001, 'caption': 'a dog is swimming in a pool'},\n",
       " {'image_id': 7002,\n",
       "  'caption': 'a man in a green shirt is carrying a baby in a market'},\n",
       " {'image_id': 7003,\n",
       "  'caption': 'a man is sitting on a bench in front of a red <unk>'},\n",
       " {'image_id': 7004, 'caption': 'a football player in a red uniform'},\n",
       " {'image_id': 7005, 'caption': 'a dog runs through the grass'},\n",
       " {'image_id': 7006, 'caption': 'a woman and a woman are smiling'},\n",
       " {'image_id': 7007,\n",
       "  'caption': 'the brown and white dog is running through the grass'},\n",
       " {'image_id': 7008, 'caption': 'a black dog is running in the sand'},\n",
       " {'image_id': 7009, 'caption': 'a group of men play basketball'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/captions_flickr8k_results_beam4.json', 'w') as fp:\n",
    "    json.dump(results, fp)\n",
    "    \n",
    "# with open('results/captions_flickr30k_results_beam4.json', 'w') as fp:\n",
    "#     json.dump(results, fp)\n",
    "\n",
    "# with open('results/captions_val2014_vinyals2015_results.json', 'w') as fp:\n",
    "#     json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
